启动hadoop：
hdfs namenode -format
# 启动 HDFS
$HADOOP_HOME/sbin/start-dfs.sh
$HADOOP_HOME/sbin/stop-dfs.sh
# 启动 YARN
$HADOOP_HOME/sbin/start-yarn.sh
$HADOOP_HOME/sbin/stop-yarn.sh

vT8&bS6=m

验证hadoop:
jps  # NameNode 应有 NameNode、ResourceManager
# 检查 DataNode
hdfs dfsadmin -report

y
y
y
y
y
y
y
y
y
y




hadoop出问题：https://blog.csdn.net/psp0001060/article/details/90110954
每个节点：
cd /opt/bigdata/hadoop-2.7.7/
rm -rf tmp
mkdir tmp
rm -rf dfs
mkdir dfs
cd dfs
mkdir name


cd /opt/bigdata/hadoop-2.7.7/
rm -rf tmp
mkdir tmp
rm -rf dfs
mkdir dfs
cd dfs
mkdir data



删除redis
rm -r /usr/local/redis-app/8006
mkdir /usr/local/redis-app/8005

###！！注意：zookeeper是管理kafka的，关的时候先关kafka再关zookeeper，开的时候先开zookeeper
启动zookeeper：（所有）
$ZOOKEEPER_HOME/bin/zkServer.sh start
$ZOOKEEPER_HOME/bin/zkServer.sh stop

验证zookeeper：
# 检查进程
jps  # 应有 QuorumPeerMain
# 查看集群状态
$ZOOKEEPER_HOME/bin/zkServer.sh status
# 客户端连接测试
$ZOOKEEPER_HOME/bin/zkCli.sh -server namenode:2181


启动hbase：
start-hbase.sh


启动kafka（所有）：
$KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties
$KAFKA_HOME/bin/kafka-server-stop.sh -daemon $KAFKA_HOME/config/server.properties


启动flink（独立模式）：
$FLINK_HOME/bin/start-cluster.sh
$FLINK_HOME/bin/stop-cluster.sh

vim $FLINK_HOME/bin/config.sh
/opt/bigdata/flink-1.16.0/tmp








启动问题：
stopping hbasecat:/tmp/hbase-root-master.pid: No such file or directory：先start-hbase，再stop-hbase

[root@namenode ~]#  ssh-copy-id datanode4
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/root/.ssh/id_rsa.pub"
mktemp: 无法通过模板"/root/.ssh/ssh-copy-id_id.XXXXXXXXXX" 创建文件: 权限不够
/usr/bin/ssh-copy-id: ERROR: mktemp failed
解决：
# 移除所有特殊属性（递归处理）
sudo chattr -iaR /root/.ssh
# 验证属性已移除
lsattr -R /root/.ssh
sestatus
如果是 Enforcing 状态：
bash
# 临时禁用 SELinux
setenforce 0
# 尝试删除操作
rm -rf /root/.ssh
再重新生成ras，再copy


